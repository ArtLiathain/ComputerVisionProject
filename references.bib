
@misc{bengio_generalized_2013,
	title = {Generalized {Denoising} {Auto}-{Encoders} as {Generative} {Models}},
	url = {http://arxiv.org/abs/1305.6663},
	doi = {10.48550/arXiv.1305.6663},
	abstract = {Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Yao, Li and Alain, Guillaume and Vincent, Pascal},
	month = nov,
	year = {2013},
	note = {arXiv:1305.6663 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/home/art/Zotero/storage/LUQ8PPDX/Bengio et al. - 2013 - Generalized Denoising Auto-Encoders as Generative Models.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/MWBB5BHU/1305.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/art/Zotero/storage/B3DJKX7Q/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/B9JXTBU8/2006.html:text/html},
}

@misc{thakur_efficient_2023,
	title = {Efficient {ResNets}: {Residual} {Network} {Design}},
	shorttitle = {Efficient {ResNets}},
	url = {http://arxiv.org/abs/2306.12100},
	doi = {10.48550/arXiv.2306.12100},
	abstract = {ResNets (or Residual Networks) are one of the most commonly used models for image classification tasks. In this project, we design and train a modified ResNet model for CIFAR-10 image classification. In particular, we aimed at maximizing the test accuracy on the CIFAR-10 benchmark while keeping the size of our ResNet model under the specified fixed budget of 5 million trainable parameters. Model size, typically measured as the number of trainable parameters, is important when models need to be stored on devices with limited storage capacity (e.g. IoT/edge devices). In this article, we present our residual network design which has less than 5 million parameters. We show that our ResNet achieves a test accuracy of 96.04\% on CIFAR-10 which is much higher than ResNet18 (which has greater than 11 million trainable parameters) when equipped with a number of training strategies and suitable ResNet hyperparameters. Models and code are available at https://github.com/Nikunj-Gupta/Efficient\_ResNets.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Thakur, Aditya and Chauhan, Harish and Gupta, Nikunj},
	month = jun,
	year = {2023},
	note = {arXiv:2306.12100 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/art/Zotero/storage/Y6KCEJ52/Thakur et al. - 2023 - Efficient ResNets Residual Network Design.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/Q45L4N3N/2306.html:text/html},
}

@misc{zamzam_pixelshuffler_2025,
	title = {{PixelShuffler}: {A} {Simple} {Image} {Translation} {Through} {Pixel} {Rearrangement}},
	shorttitle = {{PixelShuffler}},
	url = {http://arxiv.org/abs/2410.03021},
	doi = {10.48550/arXiv.2410.03021},
	abstract = {Image-to-image translation is a topic in computer vision that has a vast range of use cases ranging from medical image translation, such as converting MRI scans to CT scans or to other MRI contrasts, to image colorization, super-resolution, domain adaptation, and generating photorealistic images from sketches or semantic maps. Image style transfer is also a widely researched application of image-to-image translation, where the goal is to synthesize an image that combines the content of one image with the style of another. Existing state-of-the-art methods often rely on complex neural networks, including diffusion models and language models, to achieve high-quality style transfer, but these methods can be computationally expensive and intricate to implement. In this paper, we propose a novel pixel shuffle method that addresses the image-to-image translation problem generally with a specific demonstrative application in style transfer. The proposed method approaches style transfer by shuffling the pixels of the style image such that the mutual information between the shuffled image and the content image is maximized. This approach inherently preserves the colors of the style image while ensuring that the structural details of the content image are retained in the stylized output. We demonstrate that this simple and straightforward method produces results that are comparable to state-of-the-art techniques, as measured by the Learned Perceptual Image Patch Similarity (LPIPS) loss for content preservation and the Fr√©chet Inception Distance (FID) score for style similarity. Our experiments validate that the proposed pixel shuffle method achieves competitive performance with significantly reduced complexity, offering a promising alternative for efficient image style transfer, as well as a promise in usability of the method in general image-to-image translation tasks.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Zamzam, Omar},
	month = feb,
	year = {2025},
	note = {arXiv:2410.03021 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Preprint PDF:/home/art/Zotero/storage/JXWVH7C6/Zamzam - 2025 - PixelShuffler A Simple Image Translation Through Pixel Rearrangement.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/JIXSDKHQ/2410.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {Preprint PDF:/home/art/Zotero/storage/QDP24NQN/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/EMLM58DK/2010.html:text/html},
}


@misc{ho_cascaded_2021,
	title = {Cascaded {Diffusion} {Models} for {High} {Fidelity} {Image} {Generation}},
	url = {http://arxiv.org/abs/2106.15282},
	doi = {10.48550/arXiv.2106.15282},
	abstract = {We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02\% (top-1) and 84.06\% (top-5) at 256x256, outperforming VQ-VAE-2.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J. and Norouzi, Mohammad and Salimans, Tim},
	month = dec,
	year = {2021},
	note = {arXiv:2106.15282 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Snapshot:/home/art/Zotero/storage/BLAEAYBN/2106.html:text/html},
}

@misc{salimans_pixelcnn_2017,
	title = {{PixelCNN}++: {Improving} the {PixelCNN} with {Discretized} {Logistic} {Mixture} {Likelihood} and {Other} {Modifications}},
	shorttitle = {{PixelCNN}++},
	url = {http://arxiv.org/abs/1701.05517},
	doi = {10.48550/arXiv.1701.05517},
	abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
	urldate = {2025-12-14},
	publisher = {arXiv},
	author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
	month = jan,
	year = {2017},
	note = {arXiv:1701.05517 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/art/Zotero/storage/XZYLQW84/Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modificati.pdf:application/pdf;Snapshot:/home/art/Zotero/storage/ETB9HJ59/1701.html:text/html},
}
